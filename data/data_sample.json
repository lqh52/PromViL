{"dataset":"vg","text":["<grounding><phrase>beards<\/phrase>.","<grounding>We can see in the image: <phrase>beard<\/phrase>. Base on that, we can detect: <phrase>man with beard<\/phrase>.","<grounding><phrase>man with beard<\/phrase>.","<grounding>We can see in the image: <phrase>man with beard<\/phrase>. Base on that, we can detect: <phrase>man with beard holding a camera<\/phrase>."],"bboxes":[[[0.0392749245,0.548,0.0936555891,0.608]],[[0.0392749245,0.548,0.0936555891,0.608],[0.003021148,0.486,0.247734139,0.998]],[[0.003021148,0.486,0.247734139,0.998]],[[0.003021148,0.486,0.247734139,0.998],[0.003021148,0.486,0.247734139,0.998]]],"box_obj_match":[[0],[0,1],[0],[0,1]],"image_path":"\/path\/to\/VG\/images\/2413795.jpg","relation":null}
{"dataset":"vg","text":["<grounding><phrase>surfboards<\/phrase>.","<grounding><phrase>labels<\/phrase>.","<grounding>We can see in the image: <phrase>the surfboard<\/phrase>, <phrase>the label<\/phrase>. Base on that, we can detect: <phrase>the man holding the surfboard with the label<\/phrase>."],"bboxes":[[[0.226,0.48,0.588,0.7786666667]],[[0.498,0.4613333333,0.542,0.5626666667]],[[0.226,0.48,0.588,0.7786666667],[0.498,0.4613333333,0.542,0.5626666667],[0.306,0.2293333333,0.512,0.9946666667]]],"box_obj_match":[[0],[0],[0,1,2]],"image_path":"\/path\/to\/VG\/images\/2339105.jpg","relation":null}
{"dataset":"vg","text":["<grounding><phrase>tables<\/phrase>.","<grounding>We can see in the image: <phrase>the table<\/phrase>. Base on that, we can detect: <phrase>glass on the table<\/phrase>."],"bboxes":[[[0.006,0.5813253012,0.512,0.9939759036]],[[0.006,0.5813253012,0.512,0.9939759036],[0.072,0.7680722892,0.164,0.9819277108]]],"box_obj_match":[[0],[0,1]],"image_path":"\/path\/to\/VG\/images\/2340671.jpg","relation":null}
{"dataset":"vg","text":["<grounding><phrase>the man standing<\/phrase>.","<grounding><phrase>men<\/phrase>.","<grounding>We can see in the image: <phrase>the man standing<\/phrase>, <phrase>the man<\/phrase>. Base on that, we can detect: <phrase>tall street lamps next to the man standing<\/phrase>."],"bboxes":[[[0.272,0.745508982,0.404,0.9850299401]],[[0.294,0.7365269461,0.376,0.9910179641]],[[0.272,0.745508982,0.404,0.9850299401],[0.294,0.7365269461,0.376,0.9910179641],[0.444,0.4281437126,0.578,0.8892215569]]],"box_obj_match":[[0],[0],[0,1,2]],"image_path":"\/path\/to\/VG\/images\/2327740.jpg","relation":null}
{"dataset":"vg","text":["<grounding><phrase>baseball gloves<\/phrase>.","<grounding>We can see in the image: <phrase>the baseball glove<\/phrase>. Base on that, we can detect: <phrase>man with the baseball glove<\/phrase>.","<grounding><phrase>man with the baseball glove<\/phrase>.","<grounding>We can see in the image: <phrase>man with the baseball glove<\/phrase>. Base on that, we can detect: <phrase>man with the baseball glove throwing a baseball<\/phrase>."],"bboxes":[[[0.05,0.6366366366,0.08,0.6666666667]],[[0.05,0.6366366366,0.08,0.6666666667],[0.072,0.6306306306,0.132,0.7747747748]],[[0.072,0.6306306306,0.132,0.7747747748]],[[0.072,0.6306306306,0.132,0.7747747748],[0.072,0.6306306306,0.132,0.7747747748]]],"box_obj_match":[[0],[0,1],[0],[0,1]],"image_path":"\/path\/to\/VG\/images\/2390181.jpg","relation":null}
{"dataset":"llava","text":["Question: Write a detailed description of the given image. Answer: The scene features a herd of zebras and two giraffes in a game park or zoo. The zebras are scattered around the area, with some located closer to the left of the image, while others are towards the right side. The giraffes can be seen standing among the zebras, with one giraffe being closer to the left side and the other positioned in the middle of the image.\n\nSeveral thatched umbrellas are located above the animals, providing shade and shelter from the sun. The area appears to be a dirt ground, which is typical of a game park or zoo habitat.\n\nAdditionally, there are a few people and cars visible in the scene. The people and cars are likely visitors to the park, observing the animals from a safe distance."],"bboxes":null,"box_obj_match":null,"image_path":"\/path\/to\/COCO\/images\/COCO_train2014_000000299041.jpg","relation":null}
{"dataset":"llava","text":["Question: What are the key elements in this picture? Answer: The image features a breakfast scene with a white and blue plate containing two glazed donuts placed near two glasses of orange juice. There are other cups visible in the scene, suggesting a variety of drinks present for the meal. \n\nTwo persons can be seen in the background, possibly preparing to enjoy the meal or have a conversation. Additionally, there is a remote placed in the vicinity, suggesting they might be watching TV or controlling another device during their breakfast. To complete the scene, there is a bowl positioned near one of the cups in the background."],"bboxes":null,"box_obj_match":null,"image_path":"\/path\/to\/COCO\/images\/COCO_train2014_000000414849.jpg","relation":null}
{"dataset":"vsr","text":["<grounding><phrase>refrigerators<\/phrase>","We can see in the image: <phrase>the refrigerator<\/phrase>. Base on that, we can detect: <phrase>the person at the right side of the refrigerator<\/phrase>"],"bboxes":[[[0.1989583333,0.43515625,0.559375,0.88359375]],[[0.1989583333,0.43515625,0.559375,0.88359375],[0.4375,0.10078125,0.8583333333,0.86484375]]],"box_obj_match":[[0],[0,1]],"image_path":"\/path\/to\/VSR\/images\/000000294751.jpg","relation":"at the right side of"}
{"dataset":"vg_qa","text":["Question: How many water bottles are shown? Short Answer: One."],"bboxes":null,"box_obj_match":null,"image_path":"\/path\/to\/VG\/images\/2326121.jpg","relation":null}
